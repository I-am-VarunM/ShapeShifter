{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow==9.2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import tarfile\n",
    "import zipfile\n",
    "#from models.research.object_detection.utils.Pillow.src.PIL import Image\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from io import StringIO\n",
    "import six.moves.urllib as urllib\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from models.research.object_detection.utils import label_map_util\n",
    "from models.research.object_detection.utils.visualization_utils import visualize_boxes_and_labels_on_image_array\n",
    "from models.research.object_detection.core import target_assigner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OBJ_DETECT_API='~/data/'\n",
    "\n",
    "MODEL_NAME = 'faster_rcnn_inception_v2_coco_2017_11_08'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'data/mscoco_label_map.pbtxt'\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "#print(label_map)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "#print(category_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for loading and displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "def read_image(path):\n",
    "    img = PIL.Image.open(path)\n",
    "    img = np.array(img, dtype=np.uint8)\n",
    "    return img\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "\n",
    "def resize_and_convert(image, target_shape=(600, 600, 3)):\n",
    "    # Resize the image\n",
    "    resized_image = PIL.Image.fromarray(image).resize((target_shape[1], target_shape[0]))\n",
    "\n",
    "    # Keep only the RGB channels\n",
    "    rgb_image = resized_image.convert('RGB')\n",
    "\n",
    "    # Convert the RGB image to a NumPy array\n",
    "    result_image = np.array(rgb_image, dtype=np.uint8)\n",
    "\n",
    "    return result_image\n",
    "\n",
    "img = read_image('data/10.png')\n",
    "resized_img = resize_and_convert(img)\n",
    "showarray(resized_img)\n",
    "print(resized_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the mask\n",
    "There are 2 masks, one for the red part of stop sign and one for the white part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of the perturbation we want to generate\n",
    "psize = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RED_MASK = np.array(PIL.Image.fromarray(np.load('data/stop_red_mask.npy')).resize((psize, psize)))\n",
    "showarray(255*RED_MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WHITE_MASK = np.array(PIL.Image.fromarray(np.load('data/stop_white_mask.npy')).resize((psize, psize)))\n",
    "showarray(255*WHITE_MASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with inference_graph.as_default():\n",
    "    image_tensor = tf.compat.v1.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_tensor')\n",
    "    inference_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        inference_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(inference_graph_def, name='',\n",
    "                            input_map={'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':image_tensor})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining some plotting utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_detections(img, scores=None, bboxes=None, min_threshold=0):\n",
    "    if scores is None or bboxes is None:\n",
    "        \n",
    "        inference_sess = tf.compat.v1.Session(graph=inference_graph)\n",
    "\n",
    "        tensors = [ inference_graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    inference_graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    inference_graph.get_tensor_by_name('detection_classes:0'),\n",
    "                    inference_graph.get_tensor_by_name('num_detections:0'),\n",
    "                    inference_graph.get_tensor_by_name('SecondStagePostprocessor/Reshape_4:0'),\n",
    "                    inference_graph.get_tensor_by_name('SecondStagePostprocessor/convert_scores:0') ]\n",
    "\n",
    "        feed_dict = { inference_graph.get_tensor_by_name('image_tensor:0'): np.expand_dims(img, axis=0) }\n",
    "\n",
    "        nms_bboxes, nms_scores, nms_classes, num_detections, bboxes, scores = inference_sess.run(tensors,\n",
    "                                                                                                 feed_dict)\n",
    "        \n",
    "        bboxes = bboxes[0]\n",
    "        scores = scores[0]\n",
    "    \n",
    "    sorted_classes = np.argsort(scores[:, 1:], axis=1)\n",
    "    sorted_scores = scores[:, 1:].copy()\n",
    "    sorted_bboxes = bboxes.copy()\n",
    "\n",
    "    for i, ordering in enumerate(sorted_classes):\n",
    "        sorted_scores[i, :] = scores[i, ordering+1]\n",
    "        sorted_bboxes[i, :] = bboxes[i, ordering, :]\n",
    "\n",
    "    sorted_classes += 1\n",
    "\n",
    "    img_viz = visualize_boxes_and_labels_on_image_array(img.copy(), \n",
    "                                                        sorted_bboxes[:, -1, :],\n",
    "                                                        sorted_classes[:, -1].astype(np.int32),\n",
    "                                                        sorted_scores[:, -1],\n",
    "                                                        category_index,\n",
    "                                                        use_normalized_coordinates=False,\n",
    "                                                        max_boxes_to_draw=sorted_scores.shape[1],\n",
    "                                                        min_score_thresh=min_threshold,\n",
    "                                                        line_thickness=1)\n",
    "    showarray(img_viz)\n",
    "\n",
    "plot_detections(resized_img, min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     240
    ],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "class ModelContainer():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "        self.patch_shape = (psize, psize, 3)\n",
    "        self.batch_size_ = 4\n",
    "        self._make_model_and_ops(None)\n",
    "\n",
    "    def get_patch(self):\n",
    "        patch = np.round((self._run(self.clipped_patch_)+1)*(255/2.0)).astype(np.uint8)\n",
    "        patch *= RED_MASK\n",
    "        patch[patch == 0] = 255\n",
    "        return patch\n",
    "\n",
    "    def assign_patch(self, new_patch):\n",
    "        self._run(self.assign_patch_, {self.patch_placeholder_: new_patch})\n",
    "\n",
    "    def reset_patch(self):\n",
    "        self.assign_patch(np.zeros(self.patch_shape))\n",
    "          \n",
    "    def train_step(self, images, patch_transforms, second_stage_cls_labels, learning_rate=1.0,\n",
    "                   dropout=None, rpn_nms_bboxes=None, rpn_nms_indices=None, patch_loss_weight=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.second_stage_cls_labels_: second_stage_cls_labels,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices,\n",
    "                      self.learning_rate_: learning_rate }\n",
    "        \n",
    "        if patch_loss_weight is not None:\n",
    "            feed_dict[self.patch_loss_weight_] = patch_loss_weight\n",
    "        \n",
    "        tensors = [ self.train_op_,\n",
    "                    self.loss_,\n",
    "                    self.second_stage_cls_loss_, \n",
    "                    self.patch_loss_]\n",
    "\n",
    "        train_op, loss, second_stage_cls_loss, patch_loss = self._run(tensors, feed_dict, dropout=dropout)\n",
    "\n",
    "        return loss, second_stage_cls_loss, patch_loss\n",
    "    \n",
    "    def inference_rpn(self, images, patch_transforms):\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms }\n",
    "        \n",
    "        tensors = [self.rpn_nms_bboxes_,\n",
    "                   self.rpn_nms_indices_ ]\n",
    "\n",
    "        rpn_nms_bboxes, rpn_nms_indices = self._run(tensors, feed_dict)\n",
    "        \n",
    "        return rpn_nms_bboxes, rpn_nms_indices\n",
    "\n",
    "    def inference(self, images, patch_transforms, rpn_nms_bboxes=None, rpn_nms_indices=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices }\n",
    "\n",
    "        tensors = [ self.patched_input_,\n",
    "                    self.second_stage_cls_scores_,\n",
    "                    self.second_stage_loc_bboxes_ ]\n",
    "\n",
    "        patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes = self._run(tensors, feed_dict)\n",
    "        patched_imgs = patched_imgs.astype(np.uint8)\n",
    "\n",
    "        plot_detections(patched_imgs[0], scores=second_stage_cls_scores[0], bboxes=second_stage_loc_bboxes[0], min_threshold=0.2)\n",
    "        \n",
    "        return patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes\n",
    "\n",
    "    def _run(self, target, feed_dict=None, dropout=None):\n",
    "        if feed_dict is None:\n",
    "            feed_dict = {}\n",
    "         \n",
    "        if dropout is not None:\n",
    "            feed_dict[self.dropout_] = dropout\n",
    "    \n",
    "        return self.sess.run(target, feed_dict=feed_dict)\n",
    "    \n",
    "    def _make_model_and_ops(self, patch_val):\n",
    "        start = time.time()\n",
    "        with self.sess.graph.as_default():\n",
    "            tf.compat.v1.set_random_seed(1234)\n",
    "            \n",
    "            # Tensors are post-fixed with an underscore!\n",
    "            self.image_input_ = tf.compat.v1.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_input')\n",
    "            self.patch_transforms_ = tf.compat.v1.placeholder(tf.float32, shape=(None, 8), name='patch_transforms')\n",
    "\n",
    "            patch_ = tf.compat.v1.get_variable('patch', self.patch_shape, dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "            self.patch_placeholder_ = tf.compat.v1.placeholder(dtype=tf.float32, shape=self.patch_shape, name='patch_placeholder')\n",
    "            self.assign_patch_ = tf.compat.v1.assign(patch_, self.patch_placeholder_)\n",
    "            self.clipped_patch_ = tf.tanh(patch_)\n",
    "\n",
    "            self.dropout_ = tf.compat.v1.placeholder_with_default(1.0, [], name='dropout')\n",
    "            patch_with_dropout_ = tf.compat.v1.nn.dropout(self.clipped_patch_, keep_prob=self.dropout_)\n",
    "            patched_input_ = tf.compat.v1.clip_by_value(self._random_overlay(self.image_input_, patch_with_dropout_), clip_value_min=-1.0, clip_value_max=1.0)\n",
    "            patched_input_ = tf.compat.v1.clip_by_value(tf.image.random_brightness(patched_input_, 10.0/255), -1.0, 1.0)\n",
    "            self.patched_input_ = tf.compat.v1.fake_quant_with_min_max_vars((patched_input_ + 1)*127.5, min=0, max=255)\n",
    "\n",
    "            # Create placeholders for NMS RPN inputs\n",
    "            self.rpn_nms_bboxes_placeholder_ = tf.compat.v1.placeholder(tf.float32, shape=(None, 4), name='rpn_nms_bboxes')\n",
    "            self.rpn_nms_indices_placeholder_ = tf.compat.v1.placeholder(tf.int32, shape=(None), name='rpn_nms_indices')\n",
    "\n",
    "            detection_graph_def = tf.compat.v1.GraphDef()\n",
    "            with tf.compat.v1.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                detection_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(detection_graph_def, name='detection',\n",
    "                                    input_map={\n",
    "                                               'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':self.patched_input_,\n",
    "                                               'Reshape_7:0':self.rpn_nms_bboxes_placeholder_,\n",
    "                                               'Reshape_8:0':self.rpn_nms_indices_placeholder_,\n",
    "                                              })\n",
    "\n",
    "            # Recreate tensors we just replaced in the input_map\n",
    "            self.rpn_nms_bboxes_ = tf.reshape(self.graph.get_tensor_by_name('detection/Reshape_6:0'), self.graph.get_tensor_by_name('detection/stack_3:0'), name='detection/Reshape_7')\n",
    "            self.rpn_nms_indices_ = tf.reshape(self.graph.get_tensor_by_name('detection/mul_1:0'), self.graph.get_tensor_by_name('detection/Reshape_8/shape:0'), name='detection/Reshape_8') \n",
    "\n",
    "            # Patch Loss\n",
    "            self.patch_loss_ = tf.nn.l2_loss(RED_MASK*(self.clipped_patch_ - np.tile(np.array([ 1.0, -0.9, -1]), (psize, psize, 1))))\n",
    "            self.patch_loss_weight_ = tf.compat.v1.placeholder_with_default(1.0, [], 'patch_loss_weight')\n",
    "\n",
    "            # Second-stage Class Loss\n",
    "            self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\n",
    "            second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\n",
    "            self.second_stage_cls_labels_ = tf.compat.v1.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\n",
    "            second_stage_cls_losses_ = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\n",
    "                                                                                      logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \n",
    "            second_stage_cls_losses_ = tf.reshape(second_stage_cls_losses_, (-1, self.second_stage_cls_labels_.shape[1]))\n",
    "            second_stage_cls_losses_ = tf.divide(second_stage_cls_losses_, tf.compat.v1.to_float(self.second_stage_cls_labels_.shape[1]))\n",
    "            self.second_stage_cls_loss_ = tf.reduce_sum(second_stage_cls_losses_)\n",
    "           \n",
    "            # Second-stage bounding boxes\n",
    "            self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\n",
    "    \n",
    "            # Sum of weighted losses\n",
    "            self.loss_ = self.patch_loss_*self.patch_loss_weight_ + (self.second_stage_cls_loss_)\n",
    "\n",
    "            # Train our attack by only training on the patch variable\n",
    "            self.learning_rate_ = tf.compat.v1.placeholder(tf.float32)\n",
    "            self.train_op_ = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate_).minimize(self.loss_, var_list=[patch_])\n",
    "            \n",
    "            if patch_val is not None:\n",
    "                self.assign_patch(patch_val)\n",
    "            else:\n",
    "                self.reset_patch()\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Finished loading the model, took {:.0f}s\".format(elapsed))\n",
    "    \n",
    "\n",
    "    def _random_overlay(self, imgs, patch):    \n",
    "        red_mask = RED_MASK.astype(np.float32)\n",
    "        white_mask = WHITE_MASK.astype(np.float32)\n",
    "        \n",
    "        red_mask = tf.stack([red_mask] * self.batch_size_)\n",
    "        white_mask = tf.stack([white_mask] * self.batch_size_)\n",
    "        padded_patch = tf.stack([patch] * self.batch_size_)\n",
    "        \n",
    "        white = tf.ones_like(red_mask) * 0.95\n",
    "              \n",
    "        red_mask = tfa.image.transform(red_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        white_mask = tfa.image.transform(white_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        padded_patch = tfa.image.transform(padded_patch, self.patch_transforms_, 'BILINEAR')\n",
    "\n",
    "        inverted_mask = (1 - red_mask - white_mask)\n",
    "\n",
    "        return white * white_mask + imgs * inverted_mask + padded_patch * red_mask\n",
    "    \n",
    "\n",
    "    def _transform_vector(self, width, x_shift, y_shift, im_scale, rot_in_degrees):\n",
    "        \"\"\"\n",
    "        If one row of transforms is [a0, a1, a2, b0, b1, b2, c0, c1], \n",
    "        then it maps the output point (x, y) to a transformed input point \n",
    "        (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k), \n",
    "        where k = c0 x + c1 y + 1. \n",
    "        The transforms are inverted compared to the transform mapping input points to output points.\n",
    "        \"\"\"\n",
    "\n",
    "        rot = float(rot_in_degrees) / 90. * (math.pi/2)\n",
    "\n",
    "        # Standard rotation matrix\n",
    "        # (use negative rot because tf.contrib.image.transform will do the inverse)\n",
    "        rot_matrix = np.array(\n",
    "            [[math.cos(-rot), -math.sin(-rot)],\n",
    "            [math.sin(-rot), math.cos(-rot)]]\n",
    "        )\n",
    "\n",
    "        # Scale it\n",
    "        # (use inverse scale because tf.contrib.image.transform will do the inverse)\n",
    "        inv_scale = 1. / im_scale \n",
    "        xform_matrix = rot_matrix * inv_scale\n",
    "        a0, a1 = xform_matrix[0]\n",
    "        b0, b1 = xform_matrix[1]\n",
    "\n",
    "        # At this point, the image will have been rotated around the top left corner,\n",
    "        # rather than around the center of the image. \n",
    "        #\n",
    "        # To fix this, we will see where the center of the image got sent by our transform,\n",
    "        # and then undo that as part of the translation we apply.\n",
    "        x_origin = float(width) / 2\n",
    "        y_origin = float(width) / 2\n",
    "\n",
    "        x_origin_shifted, y_origin_shifted = np.matmul(\n",
    "            xform_matrix,\n",
    "            np.array([x_origin, y_origin]),\n",
    "        )\n",
    "\n",
    "        x_origin_delta = x_origin - x_origin_shifted\n",
    "        y_origin_delta = y_origin - y_origin_shifted\n",
    "\n",
    "        # Combine our desired shifts with the rotation-induced undesirable shift\n",
    "        a2 = x_origin_delta - (x_shift/(2*im_scale))\n",
    "        b2 = y_origin_delta - (y_shift/(2*im_scale))\n",
    "\n",
    "        # Return these values in the order that tf.contrib.image.transform expects\n",
    "        return np.array([a0, a1, a2, b0, b1, b2, 0, 0]).astype(np.float32)\n",
    "\n",
    "    def generate_random_transformation(self, scale_min=0.2, scale_max=0.6, width=psize, max_rotation=20):\n",
    "        im_scale = np.random.uniform(low=scale_min, high=scale_max)\n",
    "\n",
    "        padding_after_scaling = (1-im_scale) * width\n",
    "        x_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "        y_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "\n",
    "        rot = np.random.uniform(-max_rotation, max_rotation)\n",
    "\n",
    "        return self._transform_vector(width, \n",
    "                                      x_shift=x_delta,\n",
    "                                      y_shift=y_delta,\n",
    "                                      im_scale=im_scale, \n",
    "                                      rot_in_degrees=rot)    \n",
    "\n",
    "model = ModelContainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TARGET_CLASS = 1 # target class: person\n",
    "\n",
    "def create_target_labels(scores, from_class, to_class):\n",
    "    target_labels = np.zeros_like(scores)\n",
    "    classes = np.argmax(scores[:, :, 1:], axis=2)+1\n",
    "\n",
    "    for i, _ in enumerate(classes):\n",
    "        for j, cls in enumerate(classes[i]):\n",
    "            cls = to_class # Just perturb all of them!\n",
    "            target_labels[i, j, cls] = 1\n",
    "\n",
    "    return target_labels\n",
    "\n",
    "# use half white images and half random noise images as the background images for the optimization\n",
    "white_imgs = np.ones((int(model.batch_size_ / 2), psize, psize, 3))\n",
    "noisy_imgs = np.random.rand(int(model.batch_size_ / 2), psize, psize, 3) * 2 - 1.0\n",
    "bg_imgs = np.concatenate([ noisy_imgs, white_imgs])\n",
    "\n",
    "patch_transformations = np.zeros((model.batch_size_, 8))\n",
    "for i in range(patch_transformations.shape[0]):\n",
    "    patch_transformations[i, :] = model.generate_random_transformation()\n",
    "\n",
    "_, scores, _ = model.inference(bg_imgs, patch_transformations)\n",
    "target_labels = create_target_labels(scores, 13, TARGET_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.reset_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(251):\n",
    "    try:\n",
    "        # Generate random transformations\n",
    "        for j in range(patch_transformations.shape[0]):\n",
    "            patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n",
    "\n",
    "        # Update patch according to changed labels\n",
    "        loss, second_stage_cls_loss, patch_loss = model.train_step(bg_imgs,\n",
    "                                                                   patch_transformations,\n",
    "                                                                   create_target_labels(scores, 13, random.randint(1, 30)),\n",
    "                                                                   learning_rate=1.0,\n",
    "                                                                   patch_loss_weight=0.01)\n",
    "\n",
    "        if (i % 50) == 0:\n",
    "            print ('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n",
    "            model.inference(bg_imgs, patch_transformations)\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print ('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch = model.get_patch()\n",
    "showarray(PIL.Image.fromarray(patch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch = model.get_patch()\n",
    "for j in range(patch_transformations.shape[0]):\n",
    "    patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n",
    "_ = model.inference(bg_imgs, patch_transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(patch).save('perturbation4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
